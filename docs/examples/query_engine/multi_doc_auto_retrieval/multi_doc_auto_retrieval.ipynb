{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010a246d-56ec-44b6-98a1-cca3723d784c",
   "metadata": {},
   "source": [
    "# Structured Hierarchical Retrieval\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Doing RAG well over multiple documents is hard. A general framework is given a user query, first select the relevant documents before selecting the content inside.\n",
    "\n",
    "But selecting the documents can be tough - how can we dynamically select documents based on different properties depending on the user query? \n",
    "\n",
    "In this notebook we show you our multi-document RAG architecture:\n",
    "\n",
    "- Represent each document as a concise **metadata** dictionary containing different properties: an extracted summary along with structured metadata.\n",
    "- Store this metadata dictionary as filters within a vector database.\n",
    "- Given a user query, first do **auto-retrieval** - infer the relevant semantic query and the set of filters to query this data (effectively combining text-to-SQL and semantic search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a599ce1-48b1-44f6-846e-b9d3463635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ab861-ea33-4057-8634-b3529c577d29",
   "metadata": {},
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in LlamaIndex Github issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dc9d9-668a-4046-b86a-0ab39d74c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12776eb2-c61d-4661-89a5-b7d3be2d8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GITHUB_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423120f-0dfe-4e77-aee5-3325c5d1442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 issues in the repo page 1\n",
      "Resulted in 100 documents\n",
      "Found 100 issues in the repo page 2\n",
      "Resulted in 200 documents\n",
      "Found 100 issues in the repo page 3\n",
      "Resulted in 300 documents\n",
      "Found 9 issues in the repo page 4\n",
      "Resulted in 309 documents\n",
      "No more issues found, stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_hub.github_repo_issues import (\n",
    "    GitHubRepositoryIssuesReader,\n",
    "    GitHubIssuesClient,\n",
    ")\n",
    "\n",
    "github_client = GitHubIssuesClient()\n",
    "loader = GitHubRepositoryIssuesReader(\n",
    "    github_client,\n",
    "    owner=\"run-llama\",\n",
    "    repo=\"llama_index\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "orig_docs = loader.load_data()\n",
    "\n",
    "limit = 100\n",
    "\n",
    "docs = []\n",
    "for idx, doc in enumerate(orig_docs):\n",
    "    doc.metadata[\"index_id\"] = doc.id_\n",
    "    if idx >= limit:\n",
    "        break\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f89b0-4c55-4bfe-83c8-8539b7939de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from llama_index import SummaryIndex, Document, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.async_utils import run_jobs\n",
    "\n",
    "\n",
    "async def aprocess_doc(doc, include_summary: bool = True):\n",
    "    \"\"\"Process doc.\"\"\"\n",
    "    print(f\"Processing {doc.id_}\")\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    date_tokens = metadata[\"created_at\"].split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_tokens[0])\n",
    "    month = int(date_tokens[1])\n",
    "    day = int(date_tokens[2])\n",
    "\n",
    "    assignee = (\n",
    "        \"\" if \"assignee\" not in doc.metadata else doc.metadata[\"assignee\"]\n",
    "    )\n",
    "    size = \"\"\n",
    "    if len(doc.metadata[\"labels\"]) > 0:\n",
    "        size_arr = [l for l in doc.metadata[\"labels\"] if \"size:\" in l]\n",
    "        size = size_arr[0].split(\":\")[1] if len(size_arr) > 0 else \"\"\n",
    "    new_metadata = {\n",
    "        \"state\": metadata[\"state\"],\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"assignee\": assignee,\n",
    "        \"size\": size,\n",
    "        \"index_id\": doc.id_,\n",
    "    }\n",
    "\n",
    "    # now extract out summary\n",
    "    summary_index = SummaryIndex.from_documents([doc])\n",
    "    query_str = \"Give a one-sentence concise summary of this issue.\"\n",
    "    query_engine = summary_index.as_query_engine(\n",
    "        service_context=ServiceContext.from_defaults(\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "        )\n",
    "    )\n",
    "    summary_txt = str(query_engine.query(query_str))\n",
    "\n",
    "    new_doc = Document(text=summary_txt, metadata=new_metadata)\n",
    "    return new_doc\n",
    "\n",
    "\n",
    "async def aprocess_docs(docs):\n",
    "    \"\"\"Process metadata on docs.\"\"\"\n",
    "\n",
    "    new_docs = []\n",
    "    tasks = []\n",
    "    for doc in docs:\n",
    "        task = aprocess_doc(doc)\n",
    "        tasks.append(task)\n",
    "\n",
    "    new_docs = await run_jobs(tasks, show_progress=True, workers=5)\n",
    "\n",
    "    # new_docs = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17728251-e7c8-47eb-b139-ee0a7246f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                  | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9620\n",
      "Processing 9312\n",
      "Processing 9435\n",
      "Processing 9576\n",
      "Processing 9219\n",
      "Processing 9571\n",
      "Processing 9383\n",
      "Processing 9425\n",
      "Processing 9405\n",
      "Processing 9624\n",
      "Processing 9419\n",
      "Processing 9546\n",
      "Processing 9373\n",
      "Processing 9565\n",
      "Processing 9408\n",
      "Processing 9372\n",
      "Processing 9560\n",
      "Processing 9415\n",
      "Processing 9414\n",
      "Processing 9097\n",
      "Processing 9492\n",
      "Processing 9358\n",
      "Processing 9385\n",
      "Processing 9269\n",
      "Processing 9380\n",
      "Processing 8802\n",
      "Processing 9352\n",
      "Processing 9525\n",
      "Processing 9368\n",
      "Processing 9543\n",
      "Processing 8893\n",
      "Processing 8551\n",
      "Processing 9470\n",
      "Processing 9342\n",
      "Processing 9518\n",
      "Processing 9343\n",
      "Processing 9488\n",
      "Processing 9338\n",
      "Processing 9337\n",
      "Processing 9335\n",
      "Processing 9623\n",
      "Processing 9314\n",
      "Processing 8536\n",
      "Processing 9510\n",
      "Processing 9523\n",
      "Processing 9416\n",
      "Processing 9522\n",
      "Processing 9520\n",
      "Processing 7244\n",
      "Processing 9519\n",
      "Processing 9602\n",
      "Processing 9507\n",
      "Processing 9605\n",
      "Processing 9491\n",
      "Processing 9490\n",
      "Processing 9611\n",
      "Processing 9353\n",
      "Processing 3258\n",
      "Processing 9575\n",
      "Processing 9348\n",
      "Processing 7299\n",
      "Processing 9625\n",
      "Processing 9483\n",
      "Processing 9630\n",
      "Processing 9481\n",
      "Processing 9627\n",
      "Processing 9469\n",
      "Processing 9626\n",
      "Processing 9477\n",
      "Processing 9164\n",
      "Processing 9450\n",
      "Processing 9398\n",
      "Processing 9613\n",
      "Processing 9459\n",
      "Processing 9612\n",
      "Processing 9394\n",
      "Processing 8832\n",
      "Processing 9439\n",
      "Processing 9421\n",
      "Processing 9609\n",
      "Processing 9413\n",
      "Processing 9618\n",
      "Processing 9509\n",
      "Processing 9574\n",
      "Processing 9339\n",
      "Processing 9603\n",
      "Processing 9604\n",
      "Processing 9427\n",
      "Processing 7457\n",
      "Processing 9417\n",
      "Processing 9583\n",
      "Processing 9581\n",
      "Processing 9426\n",
      "Processing 7720\n",
      "Processing 9475\n",
      "Processing 9431\n",
      "Processing 9471\n",
      "Processing 9472\n",
      "Processing 9607\n",
      "Processing 9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "new_docs = await aprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6907607-47b7-4966-9501-6c5320ec66e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'year': 2023,\n",
       " 'month': 12,\n",
       " 'day': 19,\n",
       " 'assignee': '',\n",
       " 'size': '',\n",
       " 'index_id': '9611'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[5].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773585a7-3027-4a12-9349-7320822514e0",
   "metadata": {},
   "source": [
    "## Load Data into Vector Store\n",
    "\n",
    "We load both the summarized metadata as well as the original docs into the vector database.\n",
    "1. **Summarized Metadata**: This goes into the `LlamaIndex_auto` collection.\n",
    "2. **Original Docs**: This goes into the `LlamaIndex_AutoDoc` collection.\n",
    "\n",
    "By storing both the summarized metadata as well as the original documents, we can execute our structured, hierarchical retrieval strategies.\n",
    "\n",
    "We load into a vector database that supports auto-retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7baf6-4267-4543-8a2c-7b44a8fcd017",
   "metadata": {},
   "source": [
    "### Load Summarized Metadata\n",
    "\n",
    "This goes into `LlamaIndex_auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e115070-06cc-4a6c-990f-75492a798cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbe94d-0fe4-48b1-954c-31d0f278ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# cloud\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"\")\n",
    "client = weaviate.Client(\n",
    "    \"https://<weaviate-cluster>.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")\n",
    "\n",
    "class_name = \"LlamaIndex_auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b544b-2e8e-4fde-a35f-fb77133a0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "client.schema.delete_class(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecfa7f-fc58-48a4-9452-5ab3e27f68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=class_name\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87910444-2cfb-4c47-8821-83d679486700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \"new_docs\" are concise summaries, we can directly feed them as nodes into VectorStoreIndex\n",
    "index = VectorStoreIndex(new_docs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c589b-a647-403d-8f24-6b37fe775c23",
   "metadata": {},
   "source": [
    "### Load Original Docs\n",
    "\n",
    "This goes into `LlamaIndex_AutoDoc`. \n",
    "\n",
    "In later sections we'll create N different query engines from this collection, each query engine pointing to a specific doc (effective creating doc-specific RAG pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db9dea-fa35-470d-bc33-6afc79b9455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'created_at': '2023-12-19T22:10:49Z',\n",
       " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9624',\n",
       " 'source': 'https://github.com/run-llama/llama_index/pull/9624',\n",
       " 'labels': ['size:L'],\n",
       " 'index_id': '9624'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7ea00-cede-472b-924b-c0003849cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "doc_class_name = \"LlamaIndex_AutoDoc\"\n",
    "client.schema.delete_class(doc_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfafd2a-f5ea-4751-90bc-ee23c5257e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct separate Weaviate Index with original docs. Define a separate query engine with query engine mapping to each doc id.\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=doc_class_name\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "doc_index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44433ac-3a4c-4bda-bb5b-6bbfd18c1ddf",
   "metadata": {},
   "source": [
    "## Setup Auto-Retriever\n",
    "\n",
    "In this section we setup our auto-retriever. There's a few steps that we need to perform.\n",
    "\n",
    "1. **Define the Schema**: Define the vector db schema (e.g. the metadata fields). This will be put into the LLM input prompt when it's deciding what metadata filters to infer.\n",
    "2. **Instantiate the VectorIndexAutoRetriever class**: This creates a retriever on top of our summarized metadata index, and takes in the defined schema as input.\n",
    "3. **Define a wrapper retriever**: This allows us to postprocess each node into an `IndexNode`, with an index id linking back source document. This will allow us to do recursive retrieval in the next section (which depends on IndexNode objects linking to downstream retrievers/query engines/other Nodes). **NOTE**: We are working on improving this abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c926128-c1aa-4f42-8061-b3f2df1272d0",
   "metadata": {},
   "source": [
    "### 1. Define the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01aade-d676-49db-b851-2ac62b4e53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Github Issues\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"state\",\n",
    "            description=\"Whether the issue is `open` or `closed`\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            description=\"The year issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"month\",\n",
    "            description=\"The month issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"day\",\n",
    "            description=\"The day issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"assignee\",\n",
    "            description=\"The assignee of the ticket\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"size\",\n",
    "            description=\"How big the issue is (XS, S, M, L, XL, XXL)\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a69271-e597-40c6-88aa-a755bfd75754",
   "metadata": {},
   "source": [
    "### 2. Instantiate VectorIndexAutoRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125a832-940a-44f3-be91-ad17cdfc267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexAutoRetriever\n",
    "\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    similarity_top_k=2,\n",
    "    empty_query_top_k=10,  # if only metadata filters are specified, this is the limit\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafb463-b4ff-42a8-8c6e-01ef5c618c26",
   "metadata": {},
   "source": [
    "#### Try It Out\n",
    "\n",
    "We can try out our autoretriever on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c064e-0c1b-41b5-83c4-d1c323e0a44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: \n",
      "Using filters: {'month': 12, 'day': 11}\n",
      "Number retrieved: 6\n",
      "{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9435'}\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"Tell me about some issues on 12/11\")\n",
    "print(f\"Number retrieved: {len(nodes)}\")\n",
    "print(nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47b99c-cb52-41fb-ae2c-45b3f5c2053a",
   "metadata": {},
   "source": [
    "### 3. Define a Wrapper Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc183223-975e-412f-84e8-dcc22b2797f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "from llama_index.schema import IndexNode, NodeWithScore\n",
    "\n",
    "\n",
    "class IndexAutoRetriever(BaseRetriever):\n",
    "    \"\"\"Index auto-retriever.\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: VectorIndexAutoRetriever):\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle):\n",
    "        \"\"\"Convert nodes to index node.\"\"\"\n",
    "        retrieved_nodes = self.retriever.retrieve(query_bundle)\n",
    "        new_retrieved_nodes = []\n",
    "        for retrieved_node in retrieved_nodes:\n",
    "            index_id = retrieved_node.metadata[\"index_id\"]\n",
    "            index_node = IndexNode.from_text_node(\n",
    "                retrieved_node.node, index_id=index_id\n",
    "            )\n",
    "            new_retrieved_nodes.append(\n",
    "                NodeWithScore(node=index_node, score=retrieved_node.score)\n",
    "            )\n",
    "        return new_retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970da6bd-f4db-4b4f-9394-97d567e11109",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_retriever = IndexAutoRetriever(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec557d-5660-4d33-9cb2-6dc282375175",
   "metadata": {},
   "source": [
    "## Setup Recursive Retriever\n",
    "\n",
    "Now we setup a recursive retriever over our data. A recursive retriever links each node of one retriever to another retriever, query engine, or Node.\n",
    "\n",
    "In this setup, we link each summarized metadata node to a retriever corresponding to a RAG pipeline over the corresponding document.\n",
    "\n",
    "We set this up through the following:\n",
    "\n",
    "1. **Define one retriever per document**: Put this in a dictionary\n",
    "2. **Define our recursive retriever**: Add the root retriever (the summarized metadata retriever), and add the other document-specific retrievers in the arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666aff2-256a-498d-baaf-9d88af6b120a",
   "metadata": {},
   "source": [
    "### 1. Define Per-Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e378c-e124-408f-a751-9e1340b9bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754b35d-73d5-4e6e-9b11-f0e5168d4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_dict = {}\n",
    "query_engine_dict = {}\n",
    "for doc in docs:\n",
    "    index_id = doc.metadata[\"index_id\"]\n",
    "    # filter for the specific doc id\n",
    "    filters = MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(\n",
    "                key=\"index_id\", operator=FilterOperator.EQ, value=index_id\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    retriever = doc_index.as_retriever(filters=filters)\n",
    "    query_engine = doc_index.as_query_engine(filters=filters)\n",
    "\n",
    "    retriever_dict[index_id] = retriever\n",
    "    query_engine_dict[index_id] = query_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1fc04-51d0-4e01-9741-8dfd0340c026",
   "metadata": {},
   "source": [
    "### 2. Define Recursive Retriever\n",
    "\n",
    "We can now define our recursive retriever, which will first query the summaries and then retrieve the underlying docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf57a43-2338-4521-aa42-604670593e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever\n",
    "\n",
    "# note: can pass `agents` dict as `query_engine_dict` since every agent can be used as a query engine\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": index_retriever, **retriever_dict},\n",
    "    # query_engine_dict=query_engine_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a91a51-9a2c-447e-9848-53ee0a705baa",
   "metadata": {},
   "source": [
    "## Try It Out\n",
    "\n",
    "Now we can start retrieving relevant context over Github Issues! \n",
    "\n",
    "To complete the RAG pipeline setup we'll combine our recursive retriever with our `RetrieverQueryEngine` to generate a response in addition to the retrieved nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2032ef-e776-49d8-b8d4-f8b79fbd3599",
   "metadata": {},
   "source": [
    "### Try Out Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136a40b-c615-4f32-bc98-fdc3f72f3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = recursive_retriever.retrieve(\"Tell me about some issues on 12/11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f3135-6b88-45fa-a25d-1b4e7e977489",
   "metadata": {},
   "source": [
    "If you ran the above, you should've gotten a long output in the logs. \n",
    "\n",
    "The result is the source chunks in the relevant docs. \n",
    "\n",
    "Let's look at the date attached to the source chunk (was present in the original metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804c74f-2bb5-4935-b15b-ff16ce0a7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source nodes: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'created_at': '2023-12-11T14:03:19Z',\n",
       " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9435',\n",
       " 'source': 'https://github.com/run-llama/llama_index/issues/9435',\n",
       " 'labels': ['bug', 'triage'],\n",
       " 'index_id': '9435'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of source nodes: {len(nodes)}\")\n",
    "nodes[0].node.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cba7dd-b5f5-4759-9dcf-b9ee06a7ec29",
   "metadata": {},
   "source": [
    "### Plug into `RetrieverQueryEngine`\n",
    "\n",
    "We plug into RetrieverQueryEngine to synthesize a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdaf13-71b4-43c4-b49c-8c9a109819f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(recursive_retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d52659-c297-458c-aba6-496995655ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Tell me about some issues on 12/11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591dae0-0438-4018-9732-c5aa9357938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were several issues created on 12/11. One of them is a bug where the metadata filter is not working correctly with Elastic search indexing. Another bug involves an error loading the 'punkt' module in the NLTK library. There are also a couple of feature requests, one for adding Postgres BM25 support and another for making llama-index compatible with models finetuned and hosted on modal.com. Additionally, there is a question about using the Slack Loader with large Slack channels.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450df09-f8c4-4dee-aa10-6c85c0ea362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some open issues related to agents\n",
      "\u001b[0mUsing query str: agents\n",
      "Using filters: {'state': 'open'}\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9472\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9472: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Feature Request]: Add stop words to ReAct agent\n",
      "### Feature Description\n",
      "\n",
      "The ReAct agent does not use any stop words and the current API does not allow these to be passed to the LLM API.\n",
      "When using the ReAct agent chat abstraction the LLM often will generate an entire conversation before this output is collected by llama-index and then trimmed to the first `Thought:`, `Action:` set.\n",
      "\n",
      "This is very, very slow for some models.\n",
      "\n",
      "A better approach would be to use any available stop word setting in the APIs llama-index calls, or to instead use a streaming approach and implement stop words when possible this way.\n",
      "\n",
      "Additionally stop words should be plumbed up to the chat, query, etc API. This could probably be its own issue.\n",
      "\n",
      "### Reason\n",
      "\n",
      "`ReActOutputParser` selects the first `Thought:`, `Action:` set to act on. This hides that the LLM is doing a lot of useless work.\n",
      "\n",
      "`ReActAgent` should probably inject a stop word. If you build a chat or query from this the LLM will do a lot of work before its output is truncated to the first `Thought:`, `Action:` block.\n",
      "\n",
      "### Value of Feature\n",
      "\n",
      "LLM usage is expensive and especially slow when working locally. Currently with a variety of models, the ReAct agent is very inefficient because it generates large outputs containing many `Thought:`, `Action:` blocks and truncates to the first one. It should just avoid generating these large blocks with a stop word or by using streaming if available and stopping after the first block.\n",
      "\n",
      "This would reduce cost and significantly increase speed for local inference.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9565\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9565: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Question]: Connecting to Mixtral 8x7b Chat on together.ai\n",
      "### Question Validation\n",
      "\n",
      "- [X] I have searched both the documentation and discord for an answer.\n",
      "\n",
      "### Question\n",
      "\n",
      "I am trying to connect to mistral chat model on together.ai\n",
      "\n",
      "model is defined as OpenAILike\n",
      "llm = OpenAILike(\n",
      "    model=\"DiscoResearch/DiscoLM-mixtral-8x7b-v2\",\n",
      "    api_base=\"https://api.together.xyz/v1\",\n",
      "    api_key=\"<secret key>\",\n",
      "    temperatue=0.1\n",
      ")\n",
      "\n",
      "but I am not getting any responses as I suspect that model is expecting specific prompt template.\n",
      "Anyone managed to make it work, quick sample would be appreciated ?\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about some open issues related to agents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d36e2-f4c4-4cca-a4b9-ddd4f6d2e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two open issues related to agents. The first issue is a feature request to add stop words to the ReAct agent. The issue describes that the ReAct agent does not currently use any stop words, which results in slow performance for some models. The request suggests using stop words in the APIs or implementing a streaming approach to improve efficiency. The second issue is a question about connecting to the Mistral 8x7b chat model on together.ai. The user is seeking assistance in making the model work and is looking for a sample prompt template.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0b913-0eb1-4d22-ae86-d6ae128d7eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some size S issues related to our llm integrations\n",
      "\u001b[0mUsing query str: llm integrations\n",
      "Using filters: {'size': 'S'}\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9421\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9421: Tell me about some size S issues related to our llm integrations\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Fix cleanup process in  _delete_node of document_summary\n",
      "# Description\n",
      "\n",
      "Cannot remove doc from DocumentSummaryIndex by delete_ref_doc(...)\n",
      "\n",
      "As designed, the user can remove the nodes from the document summary index according to \"doc_id\" by delete_ref_doc(...) which will call delete_nodes(...) from BaseIndex to do the work. \n",
      "<img width=\"600\" alt=\"截屏2023-12-10 16 12 57\" src=\"https://github.com/run-llama/llama_index/assets/114048/d1b804fd-9830-4dce-985f-ac39acffcb4d\">\n",
      "\n",
      "However, it passes the related node_ids instead of doc_id itself. \n",
      "<img width=\"692\" alt=\"截屏2023-12-10 16 10 15\" src=\"https://github.com/run-llama/llama_index/assets/114048/68e89cb2-d63d-40cc-adee-1dcec1ee443a\">\n",
      "\n",
      "Fixes # (issue)\n",
      "\n",
      "## Type of Change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [x] Bug fix (non-breaking change which fixes an issue)\n",
      "- [ ] New feature (non-breaking change which adds functionality)\n",
      "- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n",
      "- [ ] This change requires a documentation update\n",
      "\n",
      "# How Has This Been Tested?\n",
      "\n",
      "Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n",
      "\n",
      "- [ ] Added new unit/integration tests\n",
      "- [ ] Added new notebook (that tests end-to-end)\n",
      "- [x] I stared at the code and made sure it makes sense\n",
      "\n",
      "# Suggested Checklist:\n",
      "\n",
      "- [x] I have performed a self-review of my own code\n",
      "- [ ] I have commented my code, particularly in hard-to-understand areas\n",
      "- [ ] I have made corresponding changes to the documentation\n",
      "- [ ] I have added Google Colab support for the newly added notebooks.\n",
      "- [x] My changes generate no new warnings\n",
      "- [ ] I have added tests that prove my fix is effective or that my feature works\n",
      "- [x] New and existing unit tests pass locally with my changes\n",
      "- [ ] I ran `make format; make lint` to appease the lint gods\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9372\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9372: Tell me about some size S issues related to our llm integrations\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Proposal : Update the evaluation correctness function to be more robust.\n",
      "# Description\n",
      "\n",
      "Proposal : Update the evaluation correctness function to be more robust.\n",
      "\n",
      "When using RagEvaluatorPack on a large dataset, sometime GPT3/4 will return a malformated answer, raising an error in correctness.py and interumpting the benchmark (that could be costly).\n",
      "\n",
      "Such case emerge when the LLM :\n",
      " - prefix the answer with ```\\n```\n",
      " - do not answer correctly such as: ```I'm not sure how to evaluate this case so I will say 3.0```\n",
      " - Something in the content made the LLM go off-road\n",
      " \n",
      "To make the parsing more robust, I change the prompt to output the score in the form ```[SCORE:4.2]``` instead of only a number.\n",
      "\n",
      "I then use a regexp to retrieve the score instead of assuming first line.\n",
      "\n",
      "I use a regexp to remove the ```[SCORE:2.3]``` pattern from the llm answer to get a reasoning, without relying on line marker.\n",
      "\n",
      "## Type of Change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [x] Bug fix (non-breaking change which fixes an issue)\n",
      "- [ ] New feature (non-breaking change which adds functionality)\n",
      "- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n",
      "- [ ] This change requires a documentation update\n",
      "\n",
      "# How Has This Been Tested?\n",
      "\n",
      "Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n",
      "\n",
      "- [ ] Added new unit/integration tests\n",
      "- [ ] Added new notebook (that tests end-to-end)\n",
      "- [x] I stared at the code and made sure it makes sense\n",
      "\n",
      "# Suggested Checklist:\n",
      "\n",
      "- [x] I have performed a self-review of my own code\n",
      "- [ ] I have commented my code, particularly in hard-to-understand areas\n",
      "- [ ] I have made corresponding changes to the documentation\n",
      "- [ ] I have added Google Colab support for the newly added notebooks.\n",
      "- [x] My changes generate no new warnings\n",
      "- [ ] I have added tests that prove my fix is effective or that my feature works\n",
      "- [ ] New and existing unit tests pass locally with my changes\n",
      "- [ ] I ran `make format; make lint` to appease the lint gods\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about some size S issues related to our llm integrations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bc281-5b82-4ce8-8066-5ca94d641b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two size S issues related to the llm integrations. The first issue is about fixing the cleanup process in the _delete_node of document_summary. The second issue is a proposal to update the evaluation correctness function to be more robust. Both issues are currently open and have not been resolved yet.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a7945-0a77-4d0c-a324-f24e2398c146",
   "metadata": {},
   "source": [
    "## Concluding Thoughts\n",
    "\n",
    "This shows you how to create a structured retrieval layer over your document summaries, allowing you to dynamically pull in the relevant documents based on the user query.\n",
    "\n",
    "You may notice similarities between this and our [multi-document agents](https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html). Both architectures are aimed for powerful multi-document retrieval.\n",
    "\n",
    "The goal of this notebook is to show you how to apply structured querying in a multi-document setting. You can actually apply this auto-retrieval algorithm to our multi-agent setup too. The multi-agent setup is primarily focused on adding agentic reasoning across documents and per documents, alloinwg multi-part queries using chain-of-thought."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
