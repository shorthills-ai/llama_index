{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/managed/manage_retrieval_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Retriever Benchmark\n",
    "\n",
    "In this notebook, we will compare different Retrieval Strategies including\n",
    "* Google Semantic Retrieval\n",
    "* LlamaIndex Retrieval\n",
    "* Vectara Managed Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install \"google-ai-generativelanguage>=0.4,<=1.0\"\n",
    "%pip install torch sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Authentication Overview\n",
    "\n",
    "The Google Semantic Retriever API lets you perform semantic search on your own data. Since it's **your data**, this needs stricter access controls than API Keys. Authenticate with OAuth through service accounts or through your user credentials. This quickstart uses a simplified authentication approach for a testing environment, and service account setup are typically easier to start. For a production environment, learn about [authentication and authorization](https://developers.google.com/workspace/guides/auth-overview) before choosing the [access credentials](https://developers.google.com/workspace/guides/create-credentials#choose_the_access_credential_that_is_right_for_you) that are appropriate for your app.\n",
    "\n",
    "Demo recording for authenticating using service accounts: [Demo](https://drive.google.com/file/d/199LzrdhuuiordS15MJAxVrPKAwEJGPOh/view?usp=sharing)\n",
    "\n",
    "**Note**: At this time, the Google Generative AI Semantic Retriever API is [only available in certain regions](https://ai.google.dev/available_regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication (Option 1): OAuth using service accounts\n",
    "\n",
    "Google Auth [service accounts](https://cloud.google.com/iam/docs/service-account-overview) let an application authenticate to make authorized Google API calls. To OAuth using service accounts, follow the steps below:\n",
    "\n",
    "1. Enable the `Generative Language API`: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#1_enable_the_api)\n",
    "\n",
    "1. Create the Service Account by following the [documentation](https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount).\n",
    "\n",
    " * After creating the service account, generate a service account key.\n",
    "\n",
    "1. Upload your service account file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below.\n",
    "\n",
    "<img width=400 src=\"https://developers.generativeai.google/tutorials/images/colab_upload.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from llama_index.indices.managed.google.generativeai import (\n",
    "    GoogleIndex,\n",
    "    set_google_config,\n",
    ")\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"service_account_key.json\",\n",
    "    scopes=[\n",
    "        \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "        \"https://www.googleapis.com/auth/generative-language.retriever\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "set_google_config(auth_credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication (Option 2): OAuth using user credentials\n",
    "\n",
    "Please follow [OAuth Quickstart](https://developers.generativeai.google/tutorials/oauth_quickstart) to setup OAuth using user credentials. Below are overview of steps from the documentation that are required.\n",
    "\n",
    "1. Enable the `Generative Language API`: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#1_enable_the_api)\n",
    "\n",
    "1. Configure the OAuth consent screen: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#2_configure_the_oauth_consent_screen)\n",
    "\n",
    "1. Authorize credentials for a desktop application: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#3_authorize_credentials_for_a_desktop_application)\n",
    "  * If you want to run this notebook in Colab start by uploading your\n",
    "`client_secret*.json` file using the \"File > Upload\" option.\n",
    "\n",
    " * Rename the uploaded file to `client_secret.json` or change the variable `client_file_name` in the code below.\n",
    "\n",
    "<img width=400 src=\"https://developers.generativeai.google/tutorials/images/colab_upload.png\">\n",
    "\n",
    "\n",
    "**Note**: At this time, the Google Generative AI Semantic Retriever API is [only available in certain regions](https://developers.generativeai.google/available_regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace TODO-your-project-name with the project used in the OAuth Quickstart\n",
    "project_name = \"TODO-your-project-name\"  #  @param {type:\"string\"}\n",
    "# Replace TODO-your-email@gmail.com with the email added as a test user in the OAuth Quickstart\n",
    "email = \"ht@runllama.ai\"  #  @param {type:\"string\"}\n",
    "# Replace client_secret.json with the client_secret_* file name you uploaded.\n",
    "client_file_name = \"client_secret.json\"\n",
    "\n",
    "# IMPORTANT: Follow the instructions from the output - you must copy the command\n",
    "# to your terminal and copy the output after authentication back here.\n",
    "!gcloud config set project $project_name\n",
    "!gcloud config set account $email\n",
    "\n",
    "# NOTE: The simplified project setup in this tutorial triggers a \"Google hasn't verified this app.\" dialog.\n",
    "# This is normal, click \"Advanced\" -> \"Go to [app name] (unsafe)\"\n",
    "!gcloud auth application-default login --no-browser --client-id-file=$client_file_name --scopes=\"https://www.googleapis.com/auth/generative-language.retriever,https://www.googleapis.com/auth/cloud-platform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will provide you with a URL, which you should enter into your local browser.\n",
    "Follow the instruction to complete the authentication and authorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Paul Graham Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth for the query `\"which program did this author attend?\"`\n",
    "\n",
    "Wiki Link: https://en.wikipedia.org/wiki/Paul_Graham_(programmer)\n",
    "\n",
    "Answer from Wiki:\n",
    "\n",
    "```\n",
    "Graham and his family moved to Pittsburgh, Pennsylvania in 1968, where he later attended Gateway High School. Graham gained interest in science and mathematics from his father who was a nuclear physicist.[8]\n",
    "\n",
    "Graham received a Bachelor of Arts with a major in philosophy from Cornell University in 1986.[9][10][11] He then received a Master of Science in 1988 and a Doctor of Philosophy in 1990, both in computer science from Harvard University.[9][12]\n",
    "\n",
    "Graham has also studied painting at the Rhode Island School of Design and at the Accademia di Belle Arti in Florence.[9][12]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Semantic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.indices.managed.google.generativeai import GoogleIndex\n",
    "\n",
    "# Create a Google corpus.\n",
    "google_index = GoogleIndex.create_corpus(display_name=\"My first corpus!\")\n",
    "print(f\"Newly created corpus ID is {google_index.corpus_id}.\")\n",
    "\n",
    "# Ingestion.\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "google_index.insert_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Google index corpus from corpus_id\n",
    "# Don't need to load it again if you have already done the ingestion step\n",
    "google_index = GoogleIndex.from_corpus(corpus_id=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Semantic Retrieval: Using the default query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This author attended Cornell and Harvard.\n"
     ]
    }
   ],
   "source": [
    "query_engine = google_index.as_query_engine()\n",
    "response = query_engine.query(\"which program did this author attend?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the nodes from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** f9604355-015f-432c-9274-f08ab4d9ffef<br>**Similarity:** None<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e4b7d7lestm2<br>**Similarity:** 0.6522269248962402<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** mmshdy59as8n<br>**Similarity:** 0.6290713548660278<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Semantic Retrieval: Using `Verbose` Answer Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell University for their undergraduate studies, where they majored in Computer Science and minored in Philosophy. They then attended Harvard University for their graduate studies, where they studied Computer Science and wrote their dissertation on Lisp programming.\n"
     ]
    }
   ],
   "source": [
    "from google.ai.generativelanguage import (\n",
    "    GenerateAnswerRequest,\n",
    ")\n",
    "\n",
    "query_engine = google_index.as_query_engine(\n",
    "    # Extra parameters specific to the Google query engine.\n",
    "    temperature=0.3,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.VERBOSE,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Which program did this author attend?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 40f9cdc5-8ff8-4f46-a7f4-7c65c9da3fd1<br>**Similarity:** None<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 3b867c25-e4e8-407a-95fe-95fe8f430507<br>**Similarity:** None<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e4b7d7lestm2<br>**Similarity:** 0.6296358108520508<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** mmshdy59as8n<br>**Similarity:** 0.616265058517456<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Semantic Retrieval: Using `Abstractive` Answer Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell and Harvard.\n"
     ]
    }
   ],
   "source": [
    "from google.ai.generativelanguage import (\n",
    "    GenerateAnswerRequest,\n",
    ")\n",
    "\n",
    "query_engine = google_index.as_query_engine(\n",
    "    # Extra parameters specific to the Google query engine.\n",
    "    temperature=0.3,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Which program did this author attend?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 62f33668-0e70-4b5b-bbbf-dfebb6e5669e<br>**Similarity:** None<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e4b7d7lestm2<br>**Similarity:** 0.6296358108520508<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** mmshdy59as8n<br>**Similarity:** 0.616265058517456<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Semantic Retrieval: Using `Extractive` Answer Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell\n"
     ]
    }
   ],
   "source": [
    "from google.ai.generativelanguage import (\n",
    "    GenerateAnswerRequest,\n",
    ")\n",
    "\n",
    "query_engine = google_index.as_query_engine(\n",
    "    # Extra parameters specific to the Google query engine.\n",
    "    temperature=0.3,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.EXTRACTIVE,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Which program did this author attend?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 0dd287af-2f39-4ad7-85a8-89b1085e5a4f<br>**Similarity:** None<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e4b7d7lestm2<br>**Similarity:** 0.6296358108520508<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** mmshdy59as8n<br>**Similarity:** 0.616265058517456<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
       "\n",
       "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
       "\n",
       "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of A...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Semantic Retrieval: Advanced Retrieval with LlamaIndex Reranking and Synthesizer\n",
    "* `Gemini as Reranker` LLM\n",
    "* Or using `Sentence BERT` cross encoder for Reranking\n",
    "* Adopt `Abstractive` Answer Style for Response \n",
    "\n",
    "For the 1st example of reranking, we tried using `Gemini` as LLM for reranking the retrieved nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers.google.generativeai import (\n",
    "    GoogleTextSynthesizer,\n",
    ")\n",
    "from llama_index.vector_stores.google.generativeai import (\n",
    "    GoogleVectorStore,\n",
    "    google_service_context,\n",
    ")\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import Gemini\n",
    "from llama_index.postprocessor import LLMRerank\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Set up the query engine with a LLM as reranker.\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\n",
    ")\n",
    "\n",
    "embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "reranker = LLMRerank(\n",
    "    top_n=5,\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=Gemini(api_key=GOOGLE_API_KEY), embed_model=embed_model\n",
    "    ),\n",
    ")\n",
    "retriever = google_index.as_retriever(similarity_top_k=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "# Query for better result!\n",
    "response = query_engine.query(\"Which program did this author attend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell, Harvard, RISD, and the Accademia di Belli Arti in Florence.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the 2nd example of reranking, we use `SentenceTransformer` for cross-encoder reranking the retrieved nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "sbert_rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers.google.generativeai import (\n",
    "    GoogleTextSynthesizer,\n",
    ")\n",
    "from llama_index.vector_stores.google.generativeai import (\n",
    "    GoogleVectorStore,\n",
    "    google_service_context,\n",
    ")\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import Gemini\n",
    "from llama_index.postprocessor import LLMRerank\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings import GeminiEmbedding\n",
    "\n",
    "\n",
    "# Set up the query engine with a LLM as reranker.\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.1, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\n",
    ")\n",
    "\n",
    "retriever = google_index.as_retriever(similarity_top_k=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[sbert_rerank],\n",
    ")\n",
    "\n",
    "# Query for better result!\n",
    "response = query_engine.query(\"Which program did this author attend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This author attended Cornell, Harvard, RISD, and the Accademia di Belli Arti in Florence.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Observation` for `Google Semantic Retrieval`\n",
    "* `Google Semantic Retrieval` supports different `AnswerStyle`. Different style could yield different retrieval and final synthesis results. \n",
    "* The results are mostly partly correct without reranker.\n",
    "* After applying either `Gemini as LLM` or `SBERT as cross-encoder` reranker, the results are more comprehensive and accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Default Baseline with OpenAI embedding and GPT as LLM for Synthesizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_TOKEN = \"sk-\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.vector_stores import QdrantVectorStore\n",
    "from llama_index import StorageContext\n",
    "import qdrant_client\n",
    "\n",
    "# documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_retrieval_2\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")\n",
    "qdrant_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended the Accademia di Belli Arti.\n"
     ]
    }
   ],
   "source": [
    "query_engine = qdrant_index.as_query_engine()\n",
    "response = query_engine.query(\"Which program did this author attend?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 24709216-afd5-41c4-bdcf-5adf9761b93c<br>**Similarity:** 0.8373684056731672<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9ba0e798-b802-41c5-9353-32cdb0419ff7<br>**Similarity:** 0.8259537663110694<br>**Text:** I didn't want to drop out of grad school, but how else was I going to get out? I remember when my friend Robert Morris got kicked out of Cornell for writing the internet worm of 1988, I was envious that he'd found such a spectacular way to get out of grad school.\n",
       "\n",
       "Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough along to graduate that June. I didn't have a word of my dissertation written, but in what must have been the quickest bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay \"Yes, I think so. I'll give you something to read in a few days.\"\n",
       "\n",
       "I picked applications of continuations as the topic. In retrospect I should have written about macros and embedded languages. There's a whole world there that's barely been explored. But all I wanted was to get...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite the Query to include more entities related to `program`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell University for grad school and later applied to RISD (Rhode Island School of Design) for the BFA program.\n"
     ]
    }
   ],
   "source": [
    "query_engine = qdrant_index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"Which universities or schools or programs did this author attend?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Default Configuration with LLM Reranker and Tree Summarize for Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "\n",
    "\n",
    "reranker = LLMRerank(top_n=3, service_context=service_context)\n",
    "retriever = qdrant_index.as_retriever(similarity_top_k=3)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(\n",
    "        service_context=service_context,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    ),\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Which universities or schools or programs did this author attend?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell University for grad school and later applied to RISD (Rhode Island School of Design) for the BFA program. The author also mentioned studying at the Accademia di Belli Arti in Florence.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "\n",
    "\n",
    "sbert_rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=5\n",
    ")\n",
    "retriever = qdrant_index.as_retriever(similarity_top_k=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(\n",
    "        service_context=service_context,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    ),\n",
    "    node_postprocessors=[sbert_rerank],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Which universities or schools or programs did this author attend?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Cornell University, Harvard University, RISD (Rhode Island School of Design), and the Accademia di Belli Arti in Florence.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Observation` for LlamaIndex default retrieval\n",
    "* the default query engine from LlamaIndex could only yield partly correct answer\n",
    "* With `Query Rewrite`, the results getting better.\n",
    "* With `Reranking` with top-5 retrieved results, the results get `100% accurate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectara Managed Index and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.indices import VectaraIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectara_customer_id = \"\"\n",
    "vectara_corpus_id = \"\"\n",
    "vectara_api_key = \"\"\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "vectara_index = VectaraIndex.from_documents(\n",
    "    documents,\n",
    "    vectara_customer_id=vectara_customer_id,\n",
    "    vectara_corpus_id=vectara_corpus_id,\n",
    "    vectara_api_key=vectara_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author mentioned attending a program at Cornell University [4], where they pursued their undergraduate studies. They also mentioned being accepted to Harvard [3] for their graduate studies. Additionally, the author applied to RISD [5] but ended up attending and doing the foundation program at RISD in the BFA program. However, it is unclear which specific program they attended at Cornell or Harvard.\n"
     ]
    }
   ],
   "source": [
    "vectara_query_engine = vectara_index.as_query_engine(similarity_top_k=5)\n",
    "response = vectara_query_engine.query(\"Which program did this author attend?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 97ffee3f12b053ee7e3aa5a4b514a8d79cb6c698fb4aa563046a86afce81d1d19cd6e93d12ba2090866419ec9c873f0980c4de6f6e150c3f001c4ee8d6106fdd<br>**Similarity:** 0.88113284<br>**Text:** I knew what I was going to do. For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief — hard to imagine now, but not unique in 1985 — that it was already climbing the lower slopes of intelligence. I had gotten into a program at Cornell that didn't make you choose a major.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 97ffee3f12b053ee7e3aa5a4b514a8d79cb6c698fb4aa563046a86afce81d1d19cd6e93d12ba2090866419ec9c873f0980c4de6f6e150c3f001c4ee8d6106fdd<br>**Similarity:** 0.7821562<br>**Text:** Only Harvard accepted me, so that was where I went. I don't remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows. What these programs really showed was that there's a subset of natural language that's a formal language. But a very proper subset.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 97ffee3f12b053ee7e3aa5a4b514a8d79cb6c698fb4aa563046a86afce81d1d19cd6e93d12ba2090866419ec9c873f0980c4de6f6e150c3f001c4ee8d6106fdd<br>**Similarity:** 0.76425356<br>**Text:** I applied to two: RISD in the US, and the Accademia di Belli Arti in Florence, which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the Accademia, so off to Providence I went. I'd applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a transfer sophomore and said I had to do the foundation that summer.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 97ffee3f12b053ee7e3aa5a4b514a8d79cb6c698fb4aa563046a86afce81d1d19cd6e93d12ba2090866419ec9c873f0980c4de6f6e150c3f001c4ee8d6106fdd<br>**Similarity:** 0.7245674<br>**Text:** The subset I would build as an open source project was the new Lisp, whose parentheses I now wouldn't even have to hide. A lot of Lisp hackers dream of building a new Lisp, partly because one of the distinctive features of the language is that it has dialects, and partly, I think, because we have in our minds a Platonic form of Lisp that all existing dialects fall short of. I certainly did. So at the end of the summer Dan and I switched to working on this new dialect of Lisp, which I called Arc, in a house I bought in Cambridge. The following spring, lightning struck. I was invited to give a talk at a Lisp conference, so I gave one about how we'd used Lisp at Viaweb.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 97ffee3f12b053ee7e3aa5a4b514a8d79cb6c698fb4aa563046a86afce81d1d19cd6e93d12ba2090866419ec9c873f0980c4de6f6e150c3f001c4ee8d6106fdd<br>**Similarity:** 0.7108299<br>**Text:** Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything. So now I was in a PhD program in computer science, yet planning to be an artist, yet also genuinely in love with Lisp hacking and working away at On Lisp. In other words, like many a grad student, I was working energetically on multiple projects that were not my thesis. I didn't see a way out of this situation.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in response.source_nodes:\n",
    "    display_source_node(r, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Observation` for Vectara\n",
    "* Vectara could provide somehow accurate results with citations, but it misses `Accademia di Belle Arti in Florence`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
